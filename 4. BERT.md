- Transformer : self attention으로 구성된 인코더, RNN이나 CNN 구조 사용하지 않음

     → Attention is all you need

- Self attention의 구조
    - Attention (Q, K, Y) = Attention Value
    - Q : Query, K : Keys, V : Values
    - 입력을 각 가중치로 연산하여 Q, K, V를 구함.
    - 주어진 Q에 대해 모든 K와의 유사도를 각각 구하고, 구한 유사도를 키와 맵핑된 각각의 V에 반영한 후, V를 모두 더하여 리턴
- Mask : 어텐션에서 제외하기 위해 값을 가리는 것
    - 인코더의 마스크 : 값이 없는 값에 패널티 부여
    - 디코더의 마스크 : 미래 정보가 어텐션 점수를 받지 못하게 가림
- Self attention의 효과 : 단어 간의 연관 관계 정보를 얻을 수 있음
- Drop-out : 일부 노드의 연결을 임의로 삭제하는 것
- Normalization : 값이 원하는 범위를 벗어나지 않도록 제한함
- BERT : Bidirectional Encoder Representations from Transformers
    - 파인 튜닝 접근법 - Pre-trained Model + 하이퍼파라미터 재조정
    - Masked 언어 모델 - 마스킹된 단어를 맞추도록 모델 구성
    - 다음 문장 예측 - 마스킹된 단어 맞춤+두 개의 문장 연관성 학습
