- Transformer : self attention으로 구성된 인코더, RNN이나 CNN 구조 사용하지 않음

     → Attention is all you need

- Encoder - Multi-Head Attention + Feed Forward Neural Network
- Multi-Head Attention : 병렬적으로 Attention을 처리하는 구조, 각 Head마다 다른 Attention 결과가 나오기 때문에 앙상블과 유사한 효과를 얻음
- 인코더 내의 Self attention 구조
    - Attention (Q, K, Y) = Attention Value
    - Q : Query, K : Keys, V : Values
    1. 입력을 각 가중치로 연산하여(Input 단어의 임베딩 벡터를 가중치와 곱하여) Q, K, V를 구함.
    2. Q와 K의 내적(dot product)을 구하여 Score를 계산하고, score를 K 벡터 사이즈의 제곱근으로 나눔
    3. 나눈 값에 Softmax를 취하여 모든 Score를 양수로 만들고, 그 합을 1로 만들어 줌 - 확률값 얻음
    4. 확률값에 V벡터를 곱해줌 - 확률값이 높은 벡터(관련이 높은 벡터)에만 집중하고, 관련이 없는 단어들은 없애기 위해
    5. 계산 과정이 끝난 weighted vector를 모두 더해 Z를 구함
    - 위의 과정이 모두 끝난 벡터를 feed-forward 신경망으로 보냄
    - 참고 : [https://nlpinkorean.github.io/illustrated-transformer/](https://nlpinkorean.github.io/illustrated-transformer/)
    - [https://aimb.tistory.com/182](https://aimb.tistory.com/182)
    - <img width="418" alt="self attention" src="https://user-images.githubusercontent.com/63702924/134616762-4703090c-0cb8-4155-8c35-dd71c5b18b79.PNG">

- Decoder : Masked Self-Attention + Encoder-Decoder Self Attention + Feed Forward Neural Network
- Mask : 어텐션에서 제외하기 위해 값을 가리는 것
    - 인코더의 마스크 : 값이 없는 값에 패널티 부여(확률값이 낮은 값을 지움)
    - 디코더의 마스크 : 미래 정보가 어텐션 점수를 받지 못하게 가림 - 가려줄 토큰에 -무한대에 해당하는 값을 더해줌, softmax 함수를 취한 후의 값이 0이 되므로 토큰 숫자가 반영되지 않음
- Self attention의 효과 : 단어 간의 연관 관계 정보를 얻을 수 있음
- Overfitting을 줄이는 방법
    - Drop-out : 일부 노드의 연결을 임의로 삭제하는 것
    - Normalization : 값이 원하는 범위를 벗어나지 않도록 제한함 - feature의 스케일을 조정
        - Scaling은 단순히 값의 범위를 조절하지만 Normalization은 분포까지 조절함
        - <img width="441" alt="scaling" src="https://user-images.githubusercontent.com/63702924/134616771-c53df10c-6bf3-4826-ac48-4a23c3718d22.PNG">
        - <img width="502" alt="normalizing" src="https://user-images.githubusercontent.com/63702924/134616790-d89ead87-dcf6-455a-bf11-850d53402383.PNG">

- BERT : Bidirectional Encoder Representations from Transformers
    - 트랜스포머로 구성된 양방향 언어 표현
    - 피쳐 기반 접근법 + 파인 튜닝 접근법 사용 - Pre-trained Model + 데이터 추가 학습(파인 튜닝)
    - Masked 언어 모델 - 정해진 비율로 랜덤 단어 마스킹, 마스킹된 단어를 맞추도록 모델 구성
    - 다음 문장 예측 - 마스킹된 단어 맞춤+두 개의 문장 연관성 학습
    - 문맥이 고려된 워드 임베딩 방법으로 사용 가능
    - 입력 문장 맨 앞에 [CLS] 클래스 구분자를 넣고, 제일 마지막에 [SEP] 문장 구분자를 넣어서 학습시킴 또는 여러 문장의 경우엔 문장 사이에 [SEP]을 넣음
    - 다이나믹 임베딩 가능 - 문장 형태와 위치에 따라 동일한 단어도 다른 임베딩을 갖게 됨, 문맥을 고려할 수 있음
    - Bert Fine Tuning 구현 참고 : [https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP](https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP)
    - [https://docs.likejazz.com/bert/](https://docs.likejazz.com/bert/)
